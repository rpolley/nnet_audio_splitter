<html>
	<head>
	<title>Project Proposal</title
	</head>
	<body>
	<h1>Separation of Audio into Channels Based on Source</h1>
	
	<p>Ryan Polley</p>
	<p>David McDermott</p>
	
	<h3>Project Details</h3>
	<p>
		In many applications related to natural language processing, a common confounding factor is the presence of multiple underlying sources. Our goal is separate the individual audio sources from a composite of overlapping audio sources. We will focus on spoken audio, to further benefit existing natural language processing techniques.
	</p>
	<p>
		Our approach is to take existing databases of spoken audio sources and overlap or combine multiple sources into one. We will use the combined sources as input to our neural network then train it to output the original sources. As of now we will restrict our network to only account for two overlapping sources. We believe this simplification will make the network structure easier to implement. With an easier network structure we expect to have a greater chance of tangible results. We propose using a recurrent neural network model that digests small time samples. The RNN will map the combined audio at a specific sample to each original source for the same sample. We propose preprocessing the input with a discreet fourier transform before the RNN. We believe the frequency components of the original audio sources would have good features the neural network would learn from.
	</p>
	<p>
		A recurrent neural network was chosen because it is capable of taking advantage of relevant contextual information across multiple samples. The RNN model allows us to process data as we receive it, expanding the possible real world use-cases. We are focusing on natural language processing where concurrent processing is a must. We don't believe it would be useful to wait for the entire conversation before it could be analyzed.
	</p>
	
	<h3>Data Set Details</h3>
	<p>
		Our training data will consist of composites generated from single source audio samples taken from <a href="http://www.voxforge.org/home">the Voxforge spoken audio dataset</a>. The training dataset will be generated by two audio samples from the spoken language dataset to create the composite input, then using the selected audio samples as expected outputs for supervised training.
	</p>
	
	<h3>Relevant Papers</h3>
	<a href="https://pdfs.semanticscholar.org/66ca/368d0eb3ffc284d57b5a1549a85cbcc0477f.pdf">Single Channel Audio Source Separation using Deep Neural Network Ensembles</a>
	
	<h3>Software</h3>
	<p>
		We will use theano and numpy as libraries, for theano's linear algebra and back-propagation capabilities and numpy's discrete fourier transform functions for preprocessing.
	</p>
	
	<h3>Progress Milestones</h3>
	<h5>By Checkpoint</h5>
	<ol>
		<li>Dataset import and processing combinations</li>
		<li>Discrete fourier transform preprocessing</li>
		<li>Basic trainable RNN</li>
		<li>Basic trained RNN (proof of concept)</li>
		<li>RNN hyper parameter optimization</li>
		<li>RNN heavily trained</li>
	</ol>
	<h5>By Final Deadline</h5>
	<ol>
		<li>Compare with other implementations</li>
		<li>Collect and consolidate metrics on RNN performance</li>
		<li>Report performance findings</li>
		<li>Compare with similar projects</li>
		<li>Propose future work</li>
	</ol>
	</body>
</html>
