<html>
	<head>
	<title>Project progress</title
	</head>
	<body>
	<h1>Separation of Audio into Channels Based on Source</h1>
	
	<p>Ryan Polley</p>
	<p>David McDermott</p>
	
	<h3>Project Details</h3>
	<p>
		In many applications related to natural language processing, a common confounding factor 
		is the presence of multiple underlying sources. Our goal is separate the individual audio 
		sources from a composite of overlapping audio sources. We will focus on spoken audio, to 
		further benefit existing natural language processing techniques.
	</p>
	<p>
		Our approach is to take existing databases of spoken audio sources and overlap or combine 
		multiple sources into one. We will use the combined sources as input to our neural network 
		then train it to output a mask that can seperate the individual sources. As of now we will 
		restrict our network to only account for two overlapping sources. We believe this 
		simplification will make the network structure easier to implement. With an easier network 
		structure we expect to have a greater chance of tangible results. We propose using a 
		recurrent neural network model that digests small time samples. The input will be preprocessed
		using short time fourier transform before the RNN. We believe the frequency components of the 
		original audio sources would have good features the neural network would learn from.
	</p>
	<p>
		A recurrent neural network was chosen because it is capable of taking advantage of relevant 
		contextual information across multiple samples. The RNN model allows us to process data as 
		we receive it, expanding the possible real world use-cases. We are focusing on natural 
		language processing where concurrent processing is a must. We don't believe it would be 
		useful to wait for the entire conversation before it could be analyzed.
	</p>
	
	<p>
		Instead of the RNN producing the two output signals we will instead train the network to
		produce a time-frequency mask. The mask is a matrix of values the rows represents the number of
		distinct frequencies and the columns are the number of time segments. Our mask is trained to 
		approximate the reference sources directly. 
	</p>
	<p>
		The inputs to the RNN are the mixed signals and the output is a time-frequency mask. The 
		mask scales the mixed signal according to the contribution of each source in the mixed signal. 
		The quality of our model is evaluated with the signal to distortion ratio(SDR), the signal 
		to interference ratio (SIR) and the signal to artifact ratio (SAR). The goal is for these 
		three ratios to be large. (i.e the signal strength is greator than the noise)
	</p>
	<p>
		The cost function of our RNN is that multiplying the mask with the mixed signal should
		produce one signal and the inverse mask should produce the other signal. The RNN 
		produces the mask by sequence conversion. The audio sequence is provided in segments of
		frequency vectors then the output is a mask for each time step. After the first time
		step the RNN will have a short delay before the mask is expected.
	</p>
	<p>
		The math behind separating mixed audio sources is described here. This problem is usually
		solved in the short time Fourier transform (STFT) domain. Given a mixed signal x(t) which 
		is a mixture of two sources s1(t) and s2(t) as x(t) = s1(t)+s2(t), let X(n, f) be the STFT 
		of x(t), where t denotes the time domain, n represents the frame index and f is the 
		frequency-index of the STFT domain of the signal. This problem can be formulated as follows:
	</p>
	<p>
		X(n, f) = S1(n, f) + S2(n, f)	
	</p>
	<p>
		where S1(n, f) and S2(n, f) are the unknown STFTs of the sources in the mixed signal. The 
		magnitude spectrograms can be written in matrix form where each column n in the matrix 
		represents a spectral frame and each row f represents a frequency index as follows:
	</p>	
	<p>
		X = S1 + S2
	</p>
	<p>
		The mask is then used to scale the magnitude of the spectrogram of the mixed signal 
		according to the contribution of each source in the mixed signal as follows:
	</p>
	<p>
		S1 = M * X
		S2 = (1-M) * X
		Where * is element-wise multiplication.
	</p>
	<p>
		For trainning we calculate the difference between the output mask and the true mask.
		This difference is squared and summed up for each time step representing the total 
		cost of the network. 
	</p>
	
	<h3>Data Set Details</h3>
	<p>
		Our training data will consist of composites generated from single source audio samples taken from <a href="http://www.voxforge.org/home">the Voxforge spoken audio dataset</a>. The training dataset will be generated by two audio samples from the spoken language dataset to create the composite input, then using the selected audio samples as expected outputs for supervised training.
	</p>
	
	<h3>Relevant Papers</h3>
	<a href="https://pdfs.semanticscholar.org/66ca/368d0eb3ffc284d57b5a1549a85cbcc0477f.pdf">Single Channel Audio Source Separation using Deep Neural Network Ensembles</a>
	
	<h3>Software</h3>
	<p>
		We will use theano and numpy as libraries, for theano's linear algebra and back-propagation capabilities and numpy's discrete fourier transform functions for preprocessing.
	</p>
	
	<h3>Progress Milestones</h3>
	<h5>By Checkpoint</h5>
	<ol>
		<li>Dataset import and processing combinations</li>
		<li>Discrete fourier transform preprocessing</li>
		<li>Basic trainable RNN</li>
		<li>Basic trained RNN (proof of concept)</li>
		<li>RNN hyper parameter optimization</li>
		<li>RNN heavily trained</li>
	</ol>
	<h5>By Final Deadline</h5>
	<ol>
		<li>Compare with other implementations</li>
		<li>Collect and consolidate metrics on RNN performance</li>
		<li>Report performance findings</li>
		<li>Compare with similar projects</li>
		<li>Propose future work</li>
	</ol>
	</body>
</html>
